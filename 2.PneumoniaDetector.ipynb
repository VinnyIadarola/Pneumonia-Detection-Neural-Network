{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d77d854e",
      "metadata": {
        "id": "d77d854e"
      },
      "source": [
        "## Imports and Defines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f4ef946a",
      "metadata": {
        "id": "f4ef946a"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import tqdm\n",
        "import torch\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13844fb8",
      "metadata": {
        "id": "13844fb8"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "23016c31",
      "metadata": {
        "id": "23016c31"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = [512]\n",
        "EPOCHS = [2, 5]\n",
        "LR = [0.005, 0.01]\n",
        "MOMENTUM = [0.2, 0.9]\n",
        "WEIGHT_DECAY = [0.0005, 0.001]\n",
        "BATCH_SIZE = [25]\n",
        "\n",
        "keys = [\"image_size\", \"epochs\", \"lr\", \"momentum\", \"weight_decay\", \"batch_size\"]\n",
        "combos = product(IMAGE_SIZE, EPOCHS, LR, MOMENTUM, WEIGHT_DECAY, BATCH_SIZE)\n",
        "\n",
        "combos = [dict(zip(keys, combo)) for combo in combos]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34cb06c",
      "metadata": {
        "id": "a34cb06c"
      },
      "source": [
        "## Database Setups and Images Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e95dc1",
      "metadata": {
        "id": "76e95dc1"
      },
      "outputs": [],
      "source": [
        "def db_setup(img_size):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        BASE_PATH = \"/content/drive/MyDrive/chest_xray/processed/\"\n",
        "    else:\n",
        "        BASE_PATH = os.path.join(os.getcwd(), \"chest_xray/processed/\")\n",
        "\n",
        "\n",
        "    db_train = datasets.ImageFolder(root=BASE_PATH+'train', transform=None)\n",
        "    db_val = datasets.ImageFolder(root=BASE_PATH+'val', transform=None)\n",
        "    db_test = datasets.ImageFolder(root=BASE_PATH+'test', transform=None)\n",
        "\n",
        "\n",
        "    db_train.transform = transforms.Compose([\n",
        "        transforms.Grayscale(1),\n",
        "        transforms.RandomResizedCrop(size=[img_size, img_size], scale=(0.5,1.)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(             # Normalize using ImageNet's mean and standard deviation\n",
        "            mean=0.485,\n",
        "            std=0.225\n",
        "        )\n",
        "    ])\n",
        "\n",
        "\n",
        "    db_val.transform = transforms.Compose([\n",
        "        transforms.Grayscale(1),\n",
        "        transforms.Resize([img_size, img_size]),\n",
        "        # Resize the short side of the image to 256\n",
        "        transforms.CenterCrop([img_size, img_size]),       # Crop a center patch of the image of size 224x224\n",
        "        transforms.ToTensor(),            # Convert the image to tensor format\n",
        "        transforms.Normalize(             # Normalize using ImageNet's mean and standard deviation\n",
        "            mean= 0.406,\n",
        "            std=0.225\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    db_test.transform = transforms.Compose([\n",
        "        transforms.Grayscale(1),\n",
        "        transforms.Resize([img_size, img_size]),           # Resize the short side of the image to 256\n",
        "        transforms.CenterCrop([img_size, img_size]),       # Crop a center patch of the image of size 224x224\n",
        "        transforms.ToTensor(),            # Convert the image to tensor format\n",
        "        transforms.Normalize(             # Normalize using ImageNet's mean and standard deviation\n",
        "            mean=0.485,\n",
        "            std=0.229\n",
        "        )\n",
        "    ])\n",
        "\n",
        "\n",
        "    return db_train, db_val, db_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bec80885",
      "metadata": {
        "id": "bec80885"
      },
      "source": [
        "## Training and Testing Funcitons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb8adfb",
      "metadata": {
        "id": "7eb8adfb"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def fit_one_epoch(model, opt, loader):\n",
        "    model.train(True)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    losses, accuracies = [], []\n",
        "    for images, labels in tqdm.tqdm(loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        pred = model(images)\n",
        "        l = loss(pred, labels)\n",
        "        acc = (pred.argmax(1) == labels).float().mean()\n",
        "\n",
        "        l.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        losses.append(l.detach().item())\n",
        "        accuracies.append(acc.detach().item())\n",
        "    return np.mean(losses), np.mean(accuracies)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(model, loader):\n",
        "    model.train(False)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    accuracies, losses = [], []\n",
        "    all_preds, all_labels = [], []\n",
        "    for images, labels in tqdm.tqdm(loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        pred = model(images)\n",
        "\n",
        "        all_preds.extend(pred.argmax(1).cpu().tolist())\n",
        "        all_labels.extend(labels.tolist())\n",
        "\n",
        "        acc = (pred.argmax(1) == labels).float().mean()\n",
        "        l = loss(pred, labels)\n",
        "\n",
        "        accuracies.append(acc.detach().item())\n",
        "        losses.append(l.detach().item())\n",
        "    return np.mean(losses), np.mean(accuracies), all_preds, all_labels\n",
        "\n",
        "@torch.no_grad()\n",
        "def plot_cm(all_labels, all_preds):\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Plot\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(cmap='gray')\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    return cm\n",
        "\n",
        "def fit(model, loader_train, loader_val, epochs=50, opt=None):\n",
        "    assert opt is not None\n",
        "    hist_tr_loss, hist_val_loss, hist_tr_acc, hist_val_acc = [], [], [], []\n",
        "    for epoch in range(epochs + 1):\n",
        "        tr_l, tr_acc = fit_one_epoch(model, opt, loader_train)\n",
        "        val_l, val_acc, p, l = eval(model, loader_val)\n",
        "\n",
        "        print(f\"Finished epoch {epoch + 1} of {epochs}: Train Loss = {tr_l:.3f}  Val Loss = {val_l:.3f}   Train Acc = {tr_acc:.3f}   Val Acc = {val_acc:.3f}\", flush=True)\n",
        "        hist_tr_loss.append(tr_l)\n",
        "        hist_val_loss.append(val_l)\n",
        "        hist_tr_acc.append(tr_acc)\n",
        "        hist_val_acc.append(val_acc)\n",
        "    return hist_tr_loss, hist_val_loss, hist_tr_acc, hist_val_acc\n",
        "\n",
        "\n",
        "def plot_training_history(combo_stats):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for combo, stats in combo_stats.items():\n",
        "        c = dict(combo)\n",
        "        label = f\"lr={c['LR']}, mom={c['MOMENTUM']}, bs={c['BATCH_SIZE']}\"\n",
        "        plt.plot(stats[\"hist_tr_acc\"], label=f\"{label} train\")\n",
        "        plt.plot(stats[\"hist_val_acc\"], label=f\"{label} val\", linestyle=\"--\")\n",
        "    plt.ylim([0.4, 1.05])\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.legend(fontsize=7)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for combo, stats in combo_stats.items():\n",
        "        c = dict(combo)\n",
        "        label = f\"lr={c['LR']}, mom={c['MOMENTUM']}, bs={c['BATCH_SIZE']}\"\n",
        "        plt.plot(stats[\"hist_tr_loss\"], label=f\"{label} train\")\n",
        "        plt.plot(stats[\"hist_val_loss\"], label=f\"{label} val\", linestyle=\"--\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.legend(fontsize=7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88522eff",
      "metadata": {
        "id": "88522eff"
      },
      "source": [
        "## Fitting Different Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "953d86d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "953d86d4",
        "outputId": "41c53ae9-0a06-4bdf-f91f-9f278dbdfdc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 163/163 [1:41:20<00:00, 37.30s/it]\n",
            "100%|██████████| 36/36 [12:23<00:00, 20.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch 0 of 2: Train Loss = 0.541  Val Loss = 0.353   Train Acc = 0.830   Val Acc = 0.859\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 163/163 [1:33:10<00:00, 34.30s/it]\n",
            "100%|██████████| 36/36 [06:27<00:00, 10.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch 1 of 2: Train Loss = 0.310  Val Loss = 0.356   Train Acc = 0.862   Val Acc = 0.844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 163/163 [1:36:14<00:00, 35.43s/it]\n",
            "100%|██████████| 36/36 [06:26<00:00, 10.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch 0 of 2: Train Loss = 0.542  Val Loss = 0.670   Train Acc = 0.831   Val Acc = 0.723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 163/163 [1:36:27<00:00, 35.51s/it]\n",
            "100%|██████████| 36/36 [06:26<00:00, 10.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch 1 of 2: Train Loss = 0.306  Val Loss = 0.453   Train Acc = 0.871   Val Acc = 0.803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 163/163 [1:34:35<00:00, 34.82s/it]\n",
            "100%|██████████| 36/36 [06:05<00:00, 10.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch 0 of 2: Train Loss = 0.485  Val Loss = 0.603   Train Acc = 0.841   Val Acc = 0.700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 163/163 [1:33:52<00:00, 34.56s/it]\n",
            "100%|██████████| 36/36 [06:05<00:00, 10.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch 1 of 2: Train Loss = 0.342  Val Loss = 0.262   Train Acc = 0.868   Val Acc = 0.899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 163/163 [1:33:01<00:00, 34.24s/it]\n",
            " 19%|█▉        | 7/36 [01:06<04:36,  9.52s/it]"
          ]
        }
      ],
      "source": [
        "combo_stats = {}\n",
        "\n",
        "for combo in combos:\n",
        "\n",
        "    combo_key = tuple(combo.items())\n",
        "\n",
        "\n",
        "    db_train, db_val, db_test = db_setup(combo[\"image_size\"])\n",
        "\n",
        "    model = models.resnet18()\n",
        "    model.conv1 = nn.Conv2d(\n",
        "        in_channels=1,\n",
        "        out_channels=64,\n",
        "        kernel_size=(7, 7),\n",
        "        stride=(2, 2),\n",
        "        padding=(3, 3),\n",
        "        bias=False\n",
        "    )\n",
        "\n",
        "    loader_train = DataLoader(db_train, batch_size=combo[\"batch_size\"], shuffle=True, drop_last=True)\n",
        "    loader_val = DataLoader(db_val, batch_size=combo[\"batch_size\"], shuffle=False)\n",
        "    loader_test = DataLoader(db_test, batch_size=combo[\"batch_size\"], shuffle=False)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=combo[\"lr\"], momentum=combo[\"momentum\"], weight_decay=combo[\"weight_decay\"])\n",
        "\n",
        "    hist_tr_loss, hist_val_loss, hist_tr_acc, hist_val_acc = fit(model, loader_train, loader_val, epochs=combo[\"epochs\"], opt=opt)\n",
        "\n",
        "    combo_stats[combo_key] = {\n",
        "        \"hist_tr_loss\"   : hist_tr_loss,\n",
        "        \"hist_val_loss\" : hist_val_loss,\n",
        "        \"hist_tr_acc\"    : hist_tr_acc,\n",
        "        \"hist_val_acc\"   : hist_val_acc\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6ba411",
      "metadata": {
        "id": "ea6ba411"
      },
      "source": [
        "## Choosing Best Hyperparameter Combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d44ee3",
      "metadata": {
        "id": "a7d44ee3"
      },
      "outputs": [],
      "source": [
        "plot_training_history(combo_stats)\n",
        "\n",
        "best_combo = None\n",
        "best_acc = -1\n",
        "best_loss = None\n",
        "\n",
        "for combo, stats in combo_stats.items():\n",
        "    final_acc = stats[\"hist_test_acc\"][-1]\n",
        "    final_loss = stats[\"hist_test_loss\"][-1]\n",
        "\n",
        "    if final_acc > best_acc:\n",
        "        best_acc = final_acc\n",
        "        best_loss = final_loss\n",
        "        best_combo = combo\n",
        "\n",
        "print(f'Best Test accuracy: {best_acc*100:.2f}%')\n",
        "print(f'Best Test loss: {best_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ae24b8",
      "metadata": {
        "id": "a3ae24b8"
      },
      "source": [
        "## Refit on Best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64e051a",
      "metadata": {
        "id": "e64e051a"
      },
      "outputs": [],
      "source": [
        "db_train, db_val, db_test = db_setup(best_combo[\"image_size\"])\n",
        "\n",
        "model = models.resnet50()\n",
        "\n",
        "loader_train = DataLoader(db_train, batch_size=best_combo[\"batch_size\"], shuffle=True, drop_last=True)\n",
        "loader_val = DataLoader(db_val, batch_size=best_combo[\"batch_size\"], shuffle=False)\n",
        "loader_test = DataLoader(db_test, batch_size=best_combo[\"batch_size\"], shuffle=False)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "opt = torch.optim.SGD(model.parameters(), lr=best_combo[\"lr\"], momentum=best_combo[\"momentum\"], weight_decay=best_combo[\"weight_decay\"])\n",
        "\n",
        "fit(model, loader_train, loader_val, epochs=best_combo[\"epochs\"], opt=opt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2e5905",
      "metadata": {
        "id": "aa2e5905"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec4b182-2ce3-4a06-a550-6307247dbfee",
      "metadata": {
        "id": "9ec4b182-2ce3-4a06-a550-6307247dbfee"
      },
      "outputs": [],
      "source": [
        "\n",
        "l, acc, all_labels, all_preds = eval(model, loader_test)\n",
        "cm = plot_cm(all_labels, all_preds)\n",
        "print(cm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
